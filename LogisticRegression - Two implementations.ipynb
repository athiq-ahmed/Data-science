{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\athiq.ahmed\\AppData2\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from numpy import loadtxt, where\n",
    "from pylab import scatter, show, legend, xlabel, ylabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale larger positive and negative values to between -1,1 depending on the largest value in the data\n",
    "\n",
    "min_max_scalar = preprocessing.MinMaxScaler(feature_range=(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\athiq.ahmed\\Desktop\\Other\\Python code\\ML\\Logistic regression\\Downloads\\LogisticRegression-master\\LogisticRegression-master\\data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grade1</th>\n",
       "      <th>grade2</th>\n",
       "      <th>label;;;;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34.623660</td>\n",
       "      <td>78.024693</td>\n",
       "      <td>0;;;;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30.286711</td>\n",
       "      <td>43.894998</td>\n",
       "      <td>0;;;;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35.847409</td>\n",
       "      <td>72.902198</td>\n",
       "      <td>0;;;;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60.182599</td>\n",
       "      <td>86.308552</td>\n",
       "      <td>1;;;;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79.032736</td>\n",
       "      <td>75.344376</td>\n",
       "      <td>1;;;;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      grade1     grade2 label;;;;\n",
       "0  34.623660  78.024693     0;;;;\n",
       "1  30.286711  43.894998     0;;;;\n",
       "2  35.847409  72.902198     0;;;;\n",
       "3  60.182599  86.308552     1;;;;\n",
       "4  79.032736  75.344376     1;;;;"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n",
    "# df.grade1.min()  # 30.05\n",
    "# df.grade1.max()  # 99.82\n",
    "\n",
    "# df.grade2.min()  # 30.60\n",
    "# df.grade2.max()  # 98.86"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.86914432,  0.38930975],\n",
       "       [-0.99346735, -0.6105909 ],\n",
       "       [-0.83406432,  0.23923558],\n",
       "       [-0.13647145,  0.6320027 ],\n",
       "       [ 0.40388679,  0.31078429]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# formats the input data into two arrays, one for independant and other for dependant variables\n",
    "\n",
    "X = df[['grade1', 'grade2']]\n",
    "X = np.array(X)\n",
    "X = min_max_scalar.fit_transform(X)\n",
    "X[:5,]\n",
    "# np.min(X)\n",
    "# np.max(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
       "       0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1.,\n",
       "       0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
       "       1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = df[\"label;;;;\"].map(lambda x:float(x.rstrip(';')))\n",
    "Y = np.array(Y)\n",
    "Y\n",
    "# min(Y)\n",
    "# max(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating training and testing records\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33)\n",
    "\n",
    "len(X_train)  # 67\n",
    "len(X_test)   # 33\n",
    "len(Y_train)  # 67\n",
    "len(Y_test)   # 33\n",
    "\n",
    "# X_train[:5,]\n",
    "# X_test[:5,]\n",
    "# Y_train[:5,]\n",
    "# Y_test[:5,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score Scikit learn:  0.85\n"
     ]
    }
   ],
   "source": [
    "# train scikit learn model\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train,Y_train)\n",
    "print('Score Scikit learn: ', round(clf.score(X_test,Y_test),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  1,  2,  5, 10, 11, 14, 17, 20, 22, 23, 27, 28, 29, 32, 34, 35,\n",
       "        36, 38, 39, 41, 43, 44, 45, 53, 54, 55, 57, 61, 62, 63, 64, 65, 67,\n",
       "        70, 78, 79, 86, 89, 92], dtype=int64),)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualize data\n",
    "\n",
    "pos = where(Y==1);pos\n",
    "neg = where(Y==0);neg\n",
    "# Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x4759560780>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scatter(X[pos,0], X[pos,1], marker='o', c='b')\n",
    "scatter(X[neg,0], X[neg,1], marker='x', c='r')\n",
    "xlabel('Exam 1 score')\n",
    "ylabel('Exam 2 score')\n",
    "legend(['Not admitted', 'Admitted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sigmoid function adjusts the cost function hypothesis to adjust the algorithm proportionally for worse estimates\n",
    "def Sigmoid(z):\n",
    "    G_of_Z = float(1.0/float(1.0 + math.exp(-1.0*z)))\n",
    "    return G_of_Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "##The hypothesis is the linear combination of all the known factors x[i] and their current estimated coefficients theta[i] \n",
    "##This hypothesis will be used to calculate each instance of the Cost Function\n",
    "\n",
    "def Hypothesis(theta, X):\n",
    "    z = 0\n",
    "    for i in range(len(theta)):\n",
    "        z = z+X[i]*theta[i]\n",
    "    return Sigmoid(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "##For each member of the dataset, the result (Y) determines which variation of the cost function is used\n",
    "##The Y = 0 cost function punishes high probability estimations, and the Y = 1 it punishes low scores\n",
    "##The \"punishment\" makes the change in the gradient of ThetaCurrent - Average(CostFunction(Dataset)) greater\n",
    "def Cost_Function(X,Y,theta,m):\n",
    "    sumOfErrors = 0\n",
    "    for i in range(m):\n",
    "        xi = X[i]\n",
    "        hi = Hypothesis(theta,xi)\n",
    "        if Y[i] == 1:\n",
    "            error = Y[i] * math.log(hi)\n",
    "        elif Y[i] == 0:\n",
    "            error = (1-Y[i]) * math.log(1-hi)\n",
    "            sumOfErrors += error\n",
    "        const = -1/m\n",
    "        J = const * sumOfErrors\n",
    "        print ('cost is ', J )\n",
    "        return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "##This function creates the gradient component for each Theta value \n",
    "##The gradient is the partial derivative by Theta of the current value of theta minus \n",
    "##a \"learning speed factor aplha\" times the average of all the cost functions for that theta\n",
    "##For each Theta there is a cost function calculated for each member of the dataset\n",
    "def Cost_Function_Derivative(X,Y,theta,j,m,alpha):\n",
    "    sumErrors = 0\n",
    "    for i in range(m):\n",
    "        xi = X[i]\n",
    "        xij = xi[j]\n",
    "        hi = Hypothesis(theta,X[i])\n",
    "        error = (hi - Y[i])*xij\n",
    "        sumErrors += error\n",
    "    m = len(Y)\n",
    "    constant = float(alpha)/float(m)\n",
    "    J = constant * sumErrors\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "##For each theta, the partial differential \n",
    "##The gradient, or vector from the current point in Theta-space (each theta value is its own dimension) to the more accurate \n",
    "##point, is the vector with each dimensional component being the partial differential for each theta value\n",
    "def Gradient_Descent(X,Y,theta,m,alpha):\n",
    "    new_theta = []\n",
    "    constant = alpha/m\n",
    "    for j in range(len(theta)):\n",
    "        CFDerivative = Cost_Function_Derivative(X,Y,theta,j,m,alpha)\n",
    "        new_theta_value = theta[j] - CFDerivative\n",
    "        new_theta.append(new_theta_value)\n",
    "    return new_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "##The high level function for the LR algorithm which, for a number of steps (num_iters) finds gradients which take \n",
    "##the Theta values (coefficients of known factors) from an estimation closer (new_theta) to their \"optimum estimation\" which is the\n",
    "##set of values best representing the system in a linear combination model\n",
    "def Logistic_Regression(X,Y,alpha,theta,num_iters):\n",
    "    m = len(Y)\n",
    "    for x in range(num_iters):\n",
    "        new_theta = Gradient_Descent(X,Y,theta,m,alpha)\n",
    "        theta = new_theta\n",
    "        if x % 100 == 0:\n",
    "            #here the cost function is used to present the final hypothesis of the model in the same form for each gradient-step iteration\n",
    "            Cost_Function(X,Y,theta,m)\n",
    "            print ('theta ', theta)\t\n",
    "            print ('cost is ', Cost_Function(X,Y,theta,m))\n",
    "    Declare_Winner(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "##This method compares the accuracy of the model generated by the scikit library with the model generated by this implementation\n",
    "def Declare_Winner(theta):\n",
    "    score = 0\n",
    "    winner = \"\"\n",
    "    #first scikit LR is tested for each independent var in the dataset and its prediction is compared against the dependent var\n",
    "    #if the prediction is the same as the dataset measured value it counts as a point for thie scikit version of LR\n",
    "    scikit_score = clf.score(X_test,Y_test)\n",
    "    length = len(X_test)\n",
    "    for i in range(length):\n",
    "        prediction = round(Hypothesis(X_test[i],theta))\n",
    "        answer = Y_test[i]\n",
    "        if prediction == answer:\n",
    "            score += 1\n",
    "    #the same process is repeated for the implementation from this module and the scores compared to find the higher match-rate\n",
    "    my_score = float(score) / float(length)\n",
    "    if my_score > scikit_score:\n",
    "        print ('You won!')\n",
    "    elif my_score == scikit_score:\n",
    "        print ('Its a tie!')\n",
    "    else:\n",
    "        print ('Scikit won.. :(')\n",
    "    print ('Your score: ', my_score)\n",
    "    print ('Scikits score: ', scikit_score )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost is  0.006890168072197608\n",
      "theta  [0.015808968977217012, 0.014030982200249273]\n",
      "cost is  0.006890168072197608\n",
      "cost is  0.006890168072197608\n",
      "cost is  0.004397292227514621\n",
      "theta  [1.1446039323506159, 1.030383323481578]\n",
      "cost is  0.004397292227514621\n",
      "cost is  0.004397292227514621\n",
      "cost is  0.0033403295622074962\n",
      "theta  [1.7920198800927762, 1.6251057941038252]\n",
      "cost is  0.0033403295622074962\n",
      "cost is  0.0033403295622074962\n",
      "cost is  0.002747211194499778\n",
      "theta  [2.2378078311381255, 2.0381775708737533]\n",
      "cost is  0.002747211194499778\n",
      "cost is  0.002747211194499778\n",
      "cost is  0.002361197918475112\n",
      "theta  [2.5764517180022444, 2.35358660097723]\n",
      "cost is  0.002361197918475112\n",
      "cost is  0.002361197918475112\n",
      "cost is  0.0020872083946966843\n",
      "theta  [2.8487364478320787, 2.608155678935002]\n",
      "cost is  0.0020872083946966843\n",
      "cost is  0.0020872083946966843\n",
      "cost is  0.0018813766427206878\n",
      "theta  [3.0758031030008572, 2.8210921909376734]\n",
      "cost is  0.0018813766427206878\n",
      "cost is  0.0018813766427206878\n",
      "cost is  0.0017204241538672554\n",
      "theta  [3.2700162725064694, 3.0036648752998807]\n",
      "cost is  0.0017204241538672554\n",
      "cost is  0.0017204241538672554\n",
      "cost is  0.0015907525213542557\n",
      "theta  [3.4392392975568247, 3.163057635787686]\n",
      "cost is  0.0015907525213542557\n",
      "cost is  0.0015907525213542557\n",
      "cost is  0.0014838416490146537\n",
      "theta  [3.588788716304762, 3.3041402117668226]\n",
      "cost is  0.0014838416490146537\n",
      "cost is  0.0014838416490146537\n",
      "Scikit won.. :(\n",
      "Your score:  0.7878787878787878\n",
      "Scikits score:  0.8484848484848485\n"
     ]
    }
   ],
   "source": [
    "# These are the initial guesses for theta as well as the learning rate of the algorithm\n",
    "# A learning rate too low will not close in on the most accurate values within a reasonable number of iterations\n",
    "# An alpha too high might overshoot the accurate values or cause irratic guesses\n",
    "# Each iteration increases model accuracy but with diminishing returns, \n",
    "# and takes a signficicant coefficient times O(n)*|Theta|, n = dataset length\n",
    "initial_theta = [0,0]\n",
    "alpha = 0.1\n",
    "iterations = 1000\n",
    "Logistic_Regression(X,Y,alpha,initial_theta,iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/perborgen/LogisticRegression/blob/master/logistic.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
